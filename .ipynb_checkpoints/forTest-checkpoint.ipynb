{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import get_data, ndcg, recall\n",
    "from model_try import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116677, 20108)\n"
     ]
    }
   ],
   "source": [
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "hidden_dim = 600\n",
    "latent_dim = 200\n",
    "batch_size = 400\n",
    "beta = None\n",
    "gamma = 0.005\n",
    "lr = 5e-4\n",
    "n_epochs = 10\n",
    "enc_epochs = 3\n",
    "dec_epochs = 1\n",
    "not_alternating = False\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "data = get_data('dataset/Ori')\n",
    "train_data, valid_in_data, valid_out_data, test_in_data, test_out_data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Try\n",
    "max = 0\n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data.iloc[i][0]) > max:\n",
    "        max = len(train_data.iloc[i][0])\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_train = \n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data.iloc[i][0]) > max:\n",
    "        max = len(train_data.iloc[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "    \n",
    "    total_samples = data_in.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "    \n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "    \n",
    "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_in, data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, device, idx, data_in, data_out=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_in = data_in\n",
    "        self._data_out = data_out\n",
    "    \n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "    \n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "        \n",
    "    def get_ratings(self, is_out=False):\n",
    "        data = self._data_out if is_out else self._data_in\n",
    "        return data[self._idx]\n",
    "    \n",
    "    def get_ratings_to_dev(self, is_out=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_out).toarray()\n",
    "        ).to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
    "    metrics = deepcopy(metrics)\n",
    "    model.eval()\n",
    "    \n",
    "    for m in metrics:\n",
    "        m['score'] = []\n",
    "    \n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          data_in=data_in,\n",
    "                          data_out=data_out,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "        \n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_out = batch.get_ratings(is_out=True)\n",
    "    \n",
    "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
    "        \n",
    "        if not (data_in is data_out):\n",
    "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "            \n",
    "        for m in metrics:\n",
    "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
    "\n",
    "    for m in metrics:\n",
    "        m['score'] = np.concatenate(m['score']).mean()\n",
    "        \n",
    "    return [x['score'] for x in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
    "            loss.backward()\n",
    "            \n",
    "            for optimizer in opts:\n",
    "                optimizer.step()\n",
    "                \n",
    "        #print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'latent_dim': latent_dim,\n",
    "    'input_dim': train_data.shape[1]\n",
    "}\n",
    "metrics = [{'metric': ndcg, 'k': 100}]\n",
    "\n",
    "best_ndcg = -np.inf\n",
    "train_scores, valid_scores = [], []\n",
    "\n",
    "model = VAE(**model_kwargs).to(device)\n",
    "model_best = VAE(**model_kwargs).to(device)\n",
    "\n",
    "learning_kwargs = {\n",
    "    'model': model,\n",
    "    'train_data': train_data,\n",
    "    'batch_size': batch_size,\n",
    "    'gamma': gamma,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "print(type(best_ndcg))\n",
    "print(best_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_params = set(model.decoder.parameters())\n",
    "tecoder_params = set(model.tecoder.parameters())\n",
    "encoder_params = set(model.encoder.parameters())\n",
    "\n",
    "optimizer_encoder = optim.Adam(encoder_params, lr=lr)\n",
    "optimizer_decoder = optim.Adam(decoder_params, lr=lr)\n",
    "optimizer_tecoder = optim.Adam(tecoder_params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (fc1): Linear(in_features=20108, out_features=600, bias=True)\n",
       "    (ln1): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "    (fc2): Linear(in_features=600, out_features=600, bias=True)\n",
       "    (ln2): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "    (fc3): Linear(in_features=600, out_features=600, bias=True)\n",
       "    (ln3): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "    (fc4): Linear(in_features=600, out_features=600, bias=True)\n",
       "    (ln4): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "    (fc5): Linear(in_features=600, out_features=600, bias=True)\n",
       "    (ln5): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "    (fc_mu): Linear(in_features=600, out_features=200, bias=True)\n",
       "    (fc_logvar): Linear(in_features=600, out_features=200, bias=True)\n",
       "  )\n",
       "  (prior): CompositePrior(\n",
       "    (encoder_old): Encoder(\n",
       "      (fc1): Linear(in_features=20108, out_features=600, bias=True)\n",
       "      (ln1): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "      (fc2): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (ln2): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "      (fc3): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (ln3): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "      (fc4): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (ln4): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "      (fc5): Linear(in_features=600, out_features=600, bias=True)\n",
       "      (ln5): LayerNorm((600,), eps=0.1, elementwise_affine=True)\n",
       "      (fc_mu): Linear(in_features=600, out_features=200, bias=True)\n",
       "      (fc_logvar): Linear(in_features=600, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=200, out_features=20108, bias=True)\n",
       "  )\n",
       "  (tecoder): TEncoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (slf_attn): MultiHeadAttentionNew(\n",
       "      (w_qs): Linear(in_features=200, out_features=384, bias=False)\n",
       "      (w_ks): Linear(in_features=200, out_features=384, bias=False)\n",
       "      (w_vs): Linear(in_features=200, out_features=384, bias=False)\n",
       "      (fc): Linear(in_features=384, out_features=200, bias=False)\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (layer_norm): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (pos_ffn): PositionwiseFeedForward(\n",
       "      (w_1): Linear(in_features=200, out_features=800, bias=True)\n",
       "      (w_2): Linear(in_features=800, out_features=200, bias=True)\n",
       "      (layer_norm): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((200,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc): Linear(in_features=200, out_features=20108, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    if not_alternating:\n",
    "        run(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=0.5, **learning_kwargs)\n",
    "    else:\n",
    "        run(opts=[optimizer_encoder], n_epochs=enc_epochs, dropout_rate=0.5, beta=False, **learning_kwargs)\n",
    "        model.update_prior()\n",
    "        run(opts=[optimizer_decoder], n_epochs=dec_epochs, dropout_rate=0, beta=False, **learning_kwargs)\n",
    "        run(opts=[optimizer_tecoder], n_epochs=dec_epochs, dropout_rate=0, beta=True, **learning_kwargs)\n",
    "\n",
    "    train_scores.append(\n",
    "        evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
    "    )\n",
    "    valid_scores.append(\n",
    "        evaluate(model, valid_in_data, valid_out_data, metrics, 1)[0]\n",
    "    )\n",
    "    \n",
    "    if valid_scores[-1] > best_ndcg:\n",
    "        best_ndcg = valid_scores[-1]\n",
    "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
    "        \n",
    "\n",
    "    print(f'epoch {epoch} | valid ndcg@100: {valid_scores[-1]:.4f} | ' +\n",
    "          f'best valid: {best_ndcg:.4f} | train ndcg@100: {train_scores[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = [{'metric': ndcg, 'k': 100}, {'metric': recall, 'k': 20}, {'metric': recall, 'k': 50}]\n",
    "\n",
    "final_scores = evaluate(model_best, test_in_data, test_out_data, test_metrics)\n",
    "for metric, score in zip(test_metrics, final_scores):\n",
    "    print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
